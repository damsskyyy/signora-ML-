# -*- coding: utf-8 -*-
"""signora_realtime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FvDXXGjOtMuhui5C__Cx9RYmOQrq7vK
"""

import numpy as np
import os
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import cv2
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

pip install kaggle

# Set kaggle API credentials
os.makedirs(os.path.expanduser(r'C:\Users\damar\.kaggle'), exist_ok=True)
os.chmod(os.path.expanduser(r'C:\Users\damar\.kaggle'), 0o600)

# Download dataset using Kaggle API
!kaggle datasets download -d achmadnoer/alfabet-bisindo

# Unzip dataset
import zipfile

with zipfile.ZipFile('alfabet-bisindo.zip', 'r') as zip_ref:
    zip_ref.extractall('alfabet-bisindo')

# Load data
def load_data(data_dir):
    images = []
    labels = []
    citra_bisindo_dir = os.path.join(data_dir, 'Citra BISINDO')
    for label in os.listdir(citra_bisindo_dir):
        label_dir = os.path.join(citra_bisindo_dir, label)
        if os.path.isdir(label_dir):
            for image_file in os.listdir(label_dir):
                if image_file.endswith('.jpg') or image_file.endswith('.png'):
                    image_path = os.path.join(label_dir, image_file)
                    image = Image.open(image_path)
                    image = image.resize((150, 150))  # Resize to 150x150
                    image = np.array(image)
                    images.append(image)
                    labels.append(label)
    return np.array(images), np.array(labels)

data_dir = 'alfabet-bisindo'
images, labels = load_data(data_dir)

# Normalize images
images = preprocess_input(images)

# Encode labels to categorical values
label_to_index = {label: idx for idx, label in enumerate(sorted(set(labels)))}
index_to_label = {idx: label for label, idx in label_to_index.items()}
labels = np.array([label_to_index[label] for label in labels])
labels = to_categorical(labels)

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# Create ImageDataGenerator for data augmentation
train_datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator()

# Create data generators
train_generator = train_datagen.flow(X_train, y_train, batch_size=32)
val_generator = val_datagen.flow(X_val, y_val, batch_size=32)

# Load the pre-trained MobileNetV2 model without the top classification layer
base_model = MobileNetV2(input_shape=(150, 150, 3), include_top=False, weights='imagenet')
base_model.trainable = False  # Freeze the base model layers

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(len(label_to_index), activation='softmax')
])

# Compile the model
model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(learning_rate=0.0001),
    metrics=['accuracy']
)

# Early stopping and model checkpoint
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // 32,
    validation_data=val_generator,
    validation_steps=len(X_val) // 32,
    epochs=100,
    callbacks=[early_stopping, model_checkpoint]
)

# Load the best model
model = load_model('best_model.h5')

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(val_generator)
print(f'Validation Accuracy: {accuracy:.2f}')

# Fungsi untuk preprocessing frame
def preprocess_frame(frame):
    frame = cv2.resize(frame, (150, 150))
    frame = frame / 255.0  # Normalisasi
    frame = np.expand_dims(frame, axis=0)  # Tambahkan batch dimension
    return frame

# Inisialisasi kamera
cap = cv2.VideoCapture(0)

# Muat model yang telah dilatih
model = load_model('best_model.h5')

# Dapatkan daftar kelas
class_names = [index_to_label[i] for i in range(len(index_to_label))]

while True:
    # Tangkap frame
    ret, frame = cap.read()
    if not ret:
        break

    # Preproses frame
    preprocessed_frame = preprocess_frame(frame)

    # Prediksi huruf
    predictions = model.predict(preprocessed_frame)
    predicted_class = class_names[np.argmax(predictions)]

    # Tampilkan prediksi di frame
    cv2.putText(frame, predicted_class, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    # Tampilkan frame
    cv2.imshow('Sign Language Detector', frame)

    # Tekan 'q' untuk keluar
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Lepaskan kamera dan tutup semua jendela
cap.release()
cv2.destroyAllWindows()