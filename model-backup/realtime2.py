# -*- coding: utf-8 -*-
"""realtime2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i9kSUsTVCaeYr5iOZ-clPZl3e77Av4AH
"""

import os
import numpy as np
from PIL import Image
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.preprocessing.image import ImageDataGenerator

pip install kaggle

# Set kaggle API credentials
os.makedirs(os.path.expanduser(r'C:\Users\damar\.kaggle'), exist_ok=True)
os.chmod(os.path.expanduser(r'C:\Users\damar\.kaggle'), 0o600)

# Download dataset using Kaggle API
!kaggle datasets download -d achmadnoer/alfabet-bisindo

# Unzip dataset
import zipfile

with zipfile.ZipFile('alfabet-bisindo.zip', 'r') as zip_ref:
    zip_ref.extractall('alfabet-bisindo')

# Load data
def load_data(data_dir):
    images = []
    labels = []
    citra_bisindo_dir = os.path.join(data_dir, 'Citra BISINDO')
    for label in os.listdir(citra_bisindo_dir):
        label_dir = os.path.join(citra_bisindo_dir, label)
        if os.path.isdir(label_dir):
            for image_file in os.listdir(label_dir):
                if image_file.endswith('.jpg') or image_file.endswith('.png'):
                    image_path = os.path.join(label_dir, image_file)
                    image = Image.open(image_path)
                    image = image.resize((150, 150))  # Resize to 150x150
                    image = np.array(image)
                    images.append(image)
                    labels.append(label)
    return np.array(images), np.array(labels)

data_dir = 'alfabet-bisindo'
images, labels = load_data(data_dir)

# Image data generator for augmentation
datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    validation_split=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Load training and validation data
train_data = datagen.flow_from_directory(
    data_dir,
    target_size=(64, 64),  # resize images
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_data = datagen.flow_from_directory(
    data_dir,
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(len(train_data.class_indices), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(train_data, epochs=10, validation_data=val_data)

# Save the model
model.save('sign_language_model.h5')

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('sign_language_model.tflite', 'wb') as f:
    f.write(tflite_model)

import cv2
import numpy as np

# Load TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path="sign_language_model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Start video capture
cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Preprocess the frame
    img = cv2.resize(frame, (64, 64))
    img = np.expand_dims(img, axis=0)
    img = img.astype(np.float32) / 255.0

    # Perform inference
    interpreter.set_tensor(input_details[0]['index'], img)
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    prediction = np.argmax(output_data)

    # Display the result
    cv2.putText(frame, f'Prediction: {prediction}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow('Sign Language Detection', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()